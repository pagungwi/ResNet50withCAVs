{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e0eebbe-e25f-4711-8409-2a673ba215f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bc8f892-8c4a-4c86-bc5e-3bf418c18494",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform_image=None, transform_mask=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform_image = transform_image\n",
    "        self.transform_mask = transform_mask\n",
    "        self.images_dir = os.path.join(root_dir, 'imgs')\n",
    "        self.masks_dir = os.path.join(root_dir, 'masks')\n",
    "        self.image_filenames = os.listdir(self.images_dir)\n",
    "        \n",
    "        self.image_filenames = [name for name in self.image_filenames if not name.startswith(\".ipynb\")]\n",
    "        \n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.images_dir, self.image_filenames[idx])\n",
    "        mask_name = os.path.join(self.masks_dir, self.image_filenames[idx].replace('img_', 'mask_').replace('.jpg', '.png'))\n",
    "        \n",
    "        \n",
    "\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        mask = Image.open(mask_name)#.convert('L')  # Convert to grayscale for single channel mask\n",
    "\n",
    "        if self.transform_image:\n",
    "            image = self.transform_image(image)\n",
    "        if self.transform_mask:\n",
    "            mask = self.transform_mask(mask)\n",
    "            \n",
    "        #mask = mask.squeeze(0).long()  # Ensure mask is in long type for cross-entropy loss\n",
    "        mask = torch.squeeze(mask)  # Ensure mask is [H, W] without the channel dimension\n",
    "        \n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49b472dd-8a7c-4ad4-b5aa-5014d0a24247",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Transforms dataset\n",
    "#transform = transforms.Compose([\n",
    "#    transforms.Resize((256, 256)),  # Resize image\n",
    "#    transforms.ToTensor(),\n",
    "#])\n",
    "\n",
    "# Define Transforms\n",
    "transform_image = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "transform_mask = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40fc6484-8a64-45ed-bf0d-5d45e213484d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defines dataset and dataloaders\n",
    "train_dataset = CustomDataset(root_dir='CAVS/Main_Trail/Train', transform_image=transform_image, transform_mask=transform_mask)\n",
    "test_dataset = CustomDataset(root_dir='CAVS/Main_Trail/Test', transform_image=transform_image, transform_mask=transform_mask)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc9ac15f-6797-4c05-863e-966bcfcaf1b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define ResNet model for semantic segmentation\n",
    "class ResNetSegmentation(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNetSegmentation, self).__init__()\n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "        \n",
    "        # Remove the fully connected layer and global average pooling\n",
    "        self.resnet = nn.Sequential(*list(self.resnet.children())[:-2])\n",
    "\n",
    "        # Add 1x1 convolution layer for the final segmentation map\n",
    "        self.conv1x1 = nn.Conv2d(2048, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        x = self.conv1x1(x)\n",
    "        x = F.interpolate(x, size=(256, 256), mode='bilinear', align_corners=False)  # Upsample to (256, 256)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb4d206f-b25b-44c7-b8c0-87bf2b8bc4fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#def pixel_accuracy(output, mask):\n",
    "#    output = torch.argmax(output, dim=1)  # output: [batch_size, height, width]\n",
    "#    correct = (output == mask).float()\n",
    "#    accuracy = correct.sum() / correct.numel()\n",
    "#    return accuracy.item()\n",
    "\n",
    "def iou(pred, target, n_classes=3):\n",
    "    pred = torch.argmax(pred, dim=1)  # pred: [batch_size, height, width]\n",
    "    ious = []\n",
    "    pred = pred.view(-1)\n",
    "    target = target.view(-1)\n",
    "    for cls in range(n_classes):\n",
    "        pred_inds = pred == cls\n",
    "        target_inds = target == cls\n",
    "        intersection = (pred_inds[target_inds]).sum().float().item()\n",
    "        union = pred_inds.sum().float().item() + target_inds.sum().float().item() - intersection\n",
    "        if union != 0:\n",
    "            ious.append(intersection / union)\n",
    "    return np.mean(ious)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75f6f0cc-eee1-44ad-a07c-9e39e910acf9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peyabi/.local/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/peyabi/.local/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = ResNetSegmentation(num_classes=3)  \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1ead5a1-52c8-4379-be33-a16b13a995d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()  # Binary Cross Entropy loss for binary segmentation\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53f4ba41-d9e8-467f-af08-81d24d5ff4b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=20, device='cuda:0'):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        epoch_iou = 0\n",
    "        \n",
    "        for images, masks in train_loader:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            \n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "           # epoch_acc += pixel_accuracy(outputs, masks)\n",
    "            epoch_iou += iou(outputs, masks)\n",
    "\n",
    "        epoch_loss /= len(train_loader)\n",
    "        epoch_acc /= len(train_loader)\n",
    "        epoch_iou /= len(train_loader)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}, IoU: {epoch_iou:.4f}')\n",
    "\n",
    "        # Add validation step if val_loader is provided\n",
    "        if val_loader is not None:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for images, masks in val_loader:\n",
    "                    images = images.to(device)\n",
    "                    masks = masks.to(device)\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, masks)\n",
    "                    val_loss += loss.item() * images.size(0)\n",
    "            val_loss /= len(val_loader.dataset)\n",
    "            print(f\"Validation Loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f47be85-b7e3-4c18-939c-cecf962d2a03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "The shape of the mask [786432] at index 0 does not match the shape of the indexed tensor [262144] at index 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_model(model, train_dataloader, \u001b[38;5;28;01mNone\u001b[39;00m, criterion, optimizer, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "Cell \u001b[0;32mIn[11], line 21\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, num_epochs, device)\u001b[0m\n\u001b[1;32m     19\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     20\u001b[0m    \u001b[38;5;66;03m# epoch_acc += pixel_accuracy(outputs, masks)\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m     epoch_iou \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m iou(outputs, masks)\n\u001b[1;32m     23\u001b[0m epoch_loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n\u001b[1;32m     24\u001b[0m epoch_acc \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n",
      "Cell \u001b[0;32mIn[6], line 15\u001b[0m, in \u001b[0;36miou\u001b[0;34m(pred, target, n_classes)\u001b[0m\n\u001b[1;32m     13\u001b[0m pred_inds \u001b[38;5;241m=\u001b[39m pred \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mcls\u001b[39m\n\u001b[1;32m     14\u001b[0m target_inds \u001b[38;5;241m=\u001b[39m target \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mcls\u001b[39m\n\u001b[0;32m---> 15\u001b[0m intersection \u001b[38;5;241m=\u001b[39m (pred_inds[target_inds])\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     16\u001b[0m union \u001b[38;5;241m=\u001b[39m pred_inds\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m+\u001b[39m target_inds\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m-\u001b[39m intersection\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m union \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mIndexError\u001b[0m: The shape of the mask [786432] at index 0 does not match the shape of the indexed tensor [262144] at index 0"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_model(model, train_dataloader, None, criterion, optimizer, num_epochs=20, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56a8fdb-d0b0-4703-9ddb-4a7c28ddaa3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluation loop (example)\n",
    "model.eval()\n",
    "total_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for images, masks in test_dataloader:\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        \n",
    "        total_loss += loss.item() * images.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / len(test_dataloader.dataset)\n",
    "    print(f'Average Test Loss: {avg_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b593d014-eb9d-480b-bcf5-4891331f1a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves model\n",
    "torch.save(model.state_dict(), 'resnet_segmentation_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f75121-1a48-4d5b-a7cf-35414211b122",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_kernel",
   "language": "python",
   "name": "torch_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
