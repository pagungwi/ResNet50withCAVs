{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bbe2c0f-1d92-4a93-8142-94aab58e4e74",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_scratch/slurm.288613/ipykernel_3498191/1385814484.py:11: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use('seaborn-white')\n",
      "2024-07-15 11:37:59.250195: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-15 11:37:59.272743: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-15 11:37:59.272775: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-15 11:37:59.287240: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-15 11:38:03.329363: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "from random import randint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "plt.style.use('seaborn-white')\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from skimage.transform import resize\n",
    "\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import plot_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Dropout,BatchNormalization\n",
    "from keras.layers import Conv2D, Concatenate, MaxPooling2D\n",
    "from keras.layers import UpSampling2D, Dropout, BatchNormalization\n",
    "from tqdm import tqdm_notebook\n",
    "from keras import initializers\n",
    "from keras import regularizers\n",
    "from keras import constraints\n",
    "from tensorflow.python.keras.utils import conv_utils\n",
    "#from keras.utils.data_utils import get_file\n",
    "from tensorflow.keras.utils import get_source_inputs\n",
    "from tensorflow.keras.layers import InputSpec\n",
    "from keras import backend as K\n",
    "from keras_applications.imagenet_utils import _obtain_input_shape \n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.layers import Layer, InputSpec\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Conv2D, UpSampling2D, Conv2DTranspose\n",
    "from tensorflow.keras.layers import Activation, SpatialDropout2D\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4bc54e2-05c7-4a5d-bd27-f54745defbc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform_image=None, transform_mask=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform_image = transform_image\n",
    "        self.transform_mask = transform_mask\n",
    "        self.images_dir = os.path.join(root_dir, 'imgs')\n",
    "        self.masks_dir = os.path.join(root_dir, 'masks')\n",
    "        self.image_filenames = os.listdir(self.images_dir)\n",
    "        \n",
    "        self.image_filenames = [name for name in self.image_filenames if not name.startswith(\".ipynb\")]\n",
    "        \n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.images_dir, self.image_filenames[idx])\n",
    "        mask_name = os.path.join(self.masks_dir, self.image_filenames[idx].replace('img_', 'mask_').replace('.jpg', '.png'))\n",
    "        \n",
    "        \n",
    "\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        mask = Image.open(mask_name).convert('RGB')  # Convert to grayscale for single channel mask\n",
    "\n",
    "        if self.transform_image:\n",
    "            image = self.transform_image(image)\n",
    "        if self.transform_mask:\n",
    "            mask = self.transform_mask(mask)\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b02fb6cf-9f67-4168-83d3-76cb6600ebc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms dataset\n",
    "#transform = transforms.Compose([\n",
    "#    transforms.Resize((256, 256)),  # Resize image\n",
    "#    transforms.ToTensor(),\n",
    "#])\n",
    "\n",
    "# Define Transforms\n",
    "transform_image = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "transform_mask = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a97d63ce-fe79-442e-bcd5-0ad80ce93901",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defines dataset and dataloaders\n",
    "train_dataset = CustomDataset(root_dir='CAVS/Main_Trail/Train', transform_image=transform_image, transform_mask=transform_mask)\n",
    "test_dataset = CustomDataset(root_dir='CAVS/Main_Trail/Test', transform_image=transform_image, transform_mask=transform_mask)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09dd2782-ba55-46d1-b73d-ade47be21b24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:04<00:00,  7.91it/s]\n",
      "100%|██████████| 57/57 [00:01<00:00, 30.07it/s]\n"
     ]
    }
   ],
   "source": [
    "# Function to extract data from DataLoader\n",
    "def extract_data_from_dataloader(dataloader):\n",
    "    images_list = []\n",
    "    masks_list = []\n",
    "    \n",
    "    for images, masks in tqdm(dataloader):\n",
    "        images_list.append(images.numpy())  # Convert to numpy arrays\n",
    "        masks_list.append(masks.numpy())    # Convert to numpy arrays\n",
    "    \n",
    "    # Stack the list of arrays into a single numpy array\n",
    "    images_array = np.vstack(images_list)\n",
    "    masks_array = np.vstack(masks_list)\n",
    "    \n",
    "    return images_array, masks_array\n",
    "\n",
    "# Extract data from train and test dataloaders\n",
    "train_images, train_masks = extract_data_from_dataloader(train_dataloader)\n",
    "test_images, test_masks = extract_data_from_dataloader(test_dataloader)\n",
    "\n",
    "# Assuming your masks are integer-encoded\n",
    "#train_masks = to_categorical(train_masks, num_classes=3)\n",
    "#test_masks = to_categorical(test_masks, num_classes=3)\n",
    "\n",
    "# Create DataFrames\n",
    "train_df = pd.DataFrame({'images': list(train_images), 'masks': list(train_masks)})\n",
    "test_df = pd.DataFrame({'images': list(test_images), 'masks': list(test_masks)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64b9e194-c13d-4119-bb58-64055d23f0cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_images shape: (133, 3, 256, 256)\n",
      "train_masks shape: (133, 3, 256, 256)\n",
      "test_images shape: (57, 3, 256, 256)\n",
      "test_masks shape: (57, 3, 256, 256)\n"
     ]
    }
   ],
   "source": [
    "# Verify shapes of the data\n",
    "print(\"train_images shape:\", train_images.shape)\n",
    "print(\"train_masks shape:\", train_masks.shape)\n",
    "print(\"test_images shape:\", test_images.shape)\n",
    "print(\"test_masks shape:\", test_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f467a9f1-0291-4c8e-9acc-6510fbf3f6ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df shape: (133, 3, 256, 256)\n",
      "test_df shape: (57, 3, 256, 256)\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrames\n",
    "train_df = pd.DataFrame({'images': list(train_images), 'masks': list(train_masks)})\n",
    "test_df = pd.DataFrame({'images': list(test_images), 'masks': list(test_masks)})\n",
    "print(\"train_df shape:\", train_images.shape)\n",
    "print(\"test_df shape:\", test_masks.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e1643ed-e604-4dfe-8494-4ac182ac4ade",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_size_ori = 960\n",
    "train_df[\"coverage\"] = train_df.masks.map(np.sum) / pow(img_size_ori, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "faeb139c-e96c-4d72-aaac-6bd3f02d0597",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cov_to_class(val):    \n",
    "    for i in range(0, 11):\n",
    "        if val * 10 <= i :\n",
    "            return i\n",
    "        \n",
    "train_df[\"coverage_class\"] = train_df.coverage.map(cov_to_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5fd9ffa-e1cc-4af5-a4ba-a4e804202f77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid shape (3, 256, 256) for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m mask \u001b[38;5;241m=\u001b[39m train_df\u001b[38;5;241m.\u001b[39mloc[idx]\u001b[38;5;241m.\u001b[39mmasks\n\u001b[1;32m      8\u001b[0m ax \u001b[38;5;241m=\u001b[39m axs[\u001b[38;5;28mint\u001b[39m(i \u001b[38;5;241m/\u001b[39m grid_width), i \u001b[38;5;241m%\u001b[39m grid_width]\n\u001b[0;32m----> 9\u001b[0m ax\u001b[38;5;241m.\u001b[39mimshow(img, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGreys\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m ax\u001b[38;5;241m.\u001b[39mimshow(mask, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGreens\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#ax.text(1, img_size_ori-1, train_df.loc[idx].z, color=\"black\")\u001b[39;00m\n",
      "File \u001b[0;32m/software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages/matplotlib/__init__.py:1446\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1443\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m   1444\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1446\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(ax, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(sanitize_sequence, args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1448\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1449\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[1;32m   1450\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[0;32m/software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages/matplotlib/axes/_axes.py:5663\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5655\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_aspect(aspect)\n\u001b[1;32m   5656\u001b[0m im \u001b[38;5;241m=\u001b[39m mimage\u001b[38;5;241m.\u001b[39mAxesImage(\u001b[38;5;28mself\u001b[39m, cmap\u001b[38;5;241m=\u001b[39mcmap, norm\u001b[38;5;241m=\u001b[39mnorm,\n\u001b[1;32m   5657\u001b[0m                       interpolation\u001b[38;5;241m=\u001b[39minterpolation, origin\u001b[38;5;241m=\u001b[39morigin,\n\u001b[1;32m   5658\u001b[0m                       extent\u001b[38;5;241m=\u001b[39mextent, filternorm\u001b[38;5;241m=\u001b[39mfilternorm,\n\u001b[1;32m   5659\u001b[0m                       filterrad\u001b[38;5;241m=\u001b[39mfilterrad, resample\u001b[38;5;241m=\u001b[39mresample,\n\u001b[1;32m   5660\u001b[0m                       interpolation_stage\u001b[38;5;241m=\u001b[39minterpolation_stage,\n\u001b[1;32m   5661\u001b[0m                       \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 5663\u001b[0m im\u001b[38;5;241m.\u001b[39mset_data(X)\n\u001b[1;32m   5664\u001b[0m im\u001b[38;5;241m.\u001b[39mset_alpha(alpha)\n\u001b[1;32m   5665\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im\u001b[38;5;241m.\u001b[39mget_clip_path() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5666\u001b[0m     \u001b[38;5;66;03m# image does not already have clipping set, clip to axes patch\u001b[39;00m\n",
      "File \u001b[0;32m/software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages/matplotlib/image.py:710\u001b[0m, in \u001b[0;36m_ImageBase.set_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A[:, :, \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    708\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    709\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m]):\n\u001b[0;32m--> 710\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid shape \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m for image data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    711\u001b[0m                     \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A\u001b[38;5;241m.\u001b[39mshape))\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m    714\u001b[0m     \u001b[38;5;66;03m# If the input data has values outside the valid range (after\u001b[39;00m\n\u001b[1;32m    715\u001b[0m     \u001b[38;5;66;03m# normalisation), we issue a warning and then clip X to the bounds\u001b[39;00m\n\u001b[1;32m    716\u001b[0m     \u001b[38;5;66;03m# - otherwise casting wraps extreme values, hiding outliers and\u001b[39;00m\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;66;03m# making reliable interpretation impossible.\u001b[39;00m\n\u001b[1;32m    718\u001b[0m     high \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39missubdtype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39minteger) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid shape (3, 256, 256) for image data"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x400 with 60 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_images = 60\n",
    "grid_width = 15\n",
    "grid_height = int(max_images / grid_width)\n",
    "fig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\n",
    "for i, idx in enumerate(train_df.index[:max_images]):\n",
    "    img = train_df.loc[idx].images\n",
    "    mask = train_df.loc[idx].masks\n",
    "    ax = axs[int(i / grid_width), i % grid_width]\n",
    "    ax.imshow(img, cmap=\"Greys\")\n",
    "    ax.imshow(mask, alpha=0.3, cmap=\"Greens\")\n",
    "    #ax.text(1, img_size_ori-1, train_df.loc[idx].z, color=\"black\")\n",
    "    ax.text(img_size_ori - 1, 1, round(train_df.loc[idx].coverage, 2), color=\"black\", ha=\"right\", va=\"top\")\n",
    "    ax.text(1, 1, train_df.loc[idx].coverage_class, color=\"black\", ha=\"left\", va=\"top\")\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticklabels([])\n",
    "plt.suptitle(\"Green: salt. Top-left: coverage class, top-right: salt coverage, bottom-left: depth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b2323e-5931-4de2-9e74-b31171384d45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ids_train, ids_valid, x_train, x_valid, y_train, y_valid, cov_train, cov_test = train_test_split(\n",
    "    train_df.index.values,\n",
    "    np.array(train_df.images.tolist()), \n",
    "    np.array(train_df.masks.tolist()),\n",
    "    train_df.coverage.values,\n",
    "    test_size=0.2, stratify=train_df.coverage_class, random_state=1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1014c8b-f314-4576-ba87-321b74926a36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b0805e-d57d-4d25-a303-f780e3e8254a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Build Model\n",
    "# Define the conv_block_simple function\n",
    "def conv_block_simple(inputs, filters, name):\n",
    "    x = Conv2D(filters, (3, 3), padding='same', name=f'{name}_conv')(inputs)\n",
    "    x = BatchNormalization(name=f'{name}_bn')(x)\n",
    "    x = Activation('relu', name=f'{name}_act')(x)\n",
    "    return x\n",
    "\n",
    "def conv_block_simple_no_bn(prevlayer, filters, prefix, strides=(1, 1)):\n",
    "    conv = Conv2D(filters, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", strides=strides, name=prefix + \"_conv\")(prevlayer)\n",
    "    conv = Activation('relu', name=prefix + \"_activation\")(conv)\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28138fc0-b5a2-4b47-b915-a3f4dfb6b573",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clear previous sessions\n",
    "tf.keras.backend.clear_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07674fb3-ce97-4470-bb64-a73142da4999",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "from keras_applications.imagenet_utils import _obtain_input_shape\n",
    "from keras.layers import Input\n",
    "from keras import layers\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import AveragePooling2D\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.layers import GlobalMaxPooling2D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "#from tensorflow.keras.engine import get_source_inputs\n",
    "from tensorflow.python.keras.utils import layer_utils\n",
    "from tensorflow.keras.utils import get_file\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "\n",
    "WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels.h5'\n",
    "WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "\n",
    "\n",
    "def identity_block(input_tensor, kernel_size, filters, stage, block):\n",
    "    filters1, filters2, filters3 = filters\n",
    "    bn_axis = 3 if K.image_data_format() == 'channels_last' else 1\n",
    "\n",
    "    conv_name_base = f'res{stage}{block}_branch'\n",
    "    bn_name_base = f'bn{stage}{block}_branch'\n",
    "\n",
    "    x = Conv2D(filters1, (1, 1), name=conv_name_base + '2a')(input_tensor)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters2, kernel_size, padding='same', name=conv_name_base + '2b')(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters3, (1, 1), name=conv_name_base + '2c')(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)\n",
    "\n",
    "    x = Add()([x, input_tensor])\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def conv_block(input_tensor, kernel_size, filters, stage, block, strides=(2, 2)):\n",
    "    filters1, filters2, filters3 = filters\n",
    "    bn_axis = 3 if K.image_data_format() == 'channels_last' else 1\n",
    "\n",
    "    conv_name_base = f'res{stage}{block}_branch'\n",
    "    bn_name_base = f'bn{stage}{block}_branch'\n",
    "\n",
    "    x = Conv2D(filters1, (1, 1), strides=strides, name=conv_name_base + '2a')(input_tensor)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters2, kernel_size, padding='same', name=conv_name_base + '2b')(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters3, (1, 1), name=conv_name_base + '2c')(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)\n",
    "\n",
    "    shortcut = Conv2D(filters3, (1, 1), strides=strides, name=conv_name_base + '1')(input_tensor)\n",
    "    shortcut = BatchNormalization(axis=bn_axis, name=bn_name_base + '1')(shortcut)\n",
    "\n",
    "    x = Add()([x, shortcut])\n",
    "    x = Activation('relu')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42a640c-e25b-42af-9cd1-e653e5898e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet50_custom(input_shape=(256, 256, 3), num_classes=3):\n",
    "    base_model = ResNet50(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "\n",
    "    x = base_model.output\n",
    "    x = Conv2D(2048, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = UpSampling2D(size=(2, 2))(x)  # Upsample\n",
    "    x = Conv2D(1024, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = UpSampling2D(size=(2, 2))(x)  # Upsample\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = UpSampling2D(size=(2, 2))(x)  # Upsample\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = UpSampling2D(size=(2, 2))(x)  # Upsample\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = UpSampling2D(size=(2, 2))(x)  # Upsample\n",
    "    x = Conv2D(num_classes, (1, 1), activation='softmax')(x)  # Final layer for multi-class output\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75db68d-58ab-4721-b3a9-f5db9718c5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu6(x):\n",
    "    return K.relu(x, max_value=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9feb32da-434b-4060-aa09-b12f2aa4f395",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (256, 256, 3)\n",
    "model = ResNet50_custom(input_shape=input_shape, num_classes=3)\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01363c4f-77ef-473e-acda-4d193401681c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom functions\n",
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def generalized_dice_loss(y_true, y_pred, smooth=1):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return 1 - (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def categorical_dice_loss(y_true, y_pred):\n",
    "    return categorical_crossentropy(y_true, y_pred) + generalized_dice_loss(y_true, y_pred)\n",
    "\n",
    "def get_iou_vector(A, B):\n",
    "    batch_size = A.shape[0]\n",
    "    metric = []\n",
    "    for batch in range(batch_size):\n",
    "        t, p = A[batch], B[batch]        \n",
    "        intersection = np.logical_and(t, p)\n",
    "        union = np.logical_or(t, p)\n",
    "        iou = (np.sum(intersection > 0) + 1e-10) / (np.sum(union > 0) + 1e-10)\n",
    "        thresholds = np.arange(0.5, 1, 0.05)\n",
    "        s = []\n",
    "        for thresh in thresholds:\n",
    "            s.append(iou > thresh)\n",
    "        metric.append(np.mean(s))\n",
    "    return np.mean(metric)\n",
    "\n",
    "# Define the custom IOU metric function\n",
    "def my_iou_metric(label, pred):\n",
    "    return tf.py_function(func=get_iou_vector, inp=[label, tf.cast(pred > 0.5, tf.float32)], Tout=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef72050b-b15a-4718-a021-e5d83a6c0cce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dfc21c-99a1-4608-81b2-37d17223fddf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[my_iou_metric, 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5167a467-c8ee-4bfb-a817-8492c46a57ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_train = np.append(x_train, [np.fliplr(x) for x in x_train], axis=0)\n",
    "y_train = np.append(y_train, [np.fliplr(x) for x in y_train], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ed1829-e871-4033-bdb6-7d570b17a3a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 10, figsize=(15,3))\n",
    "for i in range(10):\n",
    "    axs[0][i].imshow(x_train[i].squeeze(), cmap=\"Greys\")\n",
    "    axs[0][i].imshow(y_train[i].squeeze(), cmap=\"Greens\", alpha=0.3)\n",
    "    axs[1][i].imshow(x_train[int(len(x_train)/2 + i)].squeeze(), cmap=\"Greys\")\n",
    "    axs[1][i].imshow(y_train[int(len(y_train)/2 + i)].squeeze(), cmap=\"Greens\", alpha=0.3)\n",
    "fig.suptitle(\"Top row: original images, bottom row: augmented images\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e286695e-c597-475b-b6d6-02d12f428490",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(\"./model.keras\", monitor='val_my_iou_metric', mode='max', save_best_only=True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(factor=0.1, patience=4, min_lr=0.00001, verbose=1)\n",
    "\n",
    "# One-hot encode the masks\n",
    "num_classes = 3\n",
    "y_train = to_categorical(y_train, num_classes=num_classes)\n",
    "y_valid = to_categorical(y_valid, num_classes=num_classes)\n",
    "# Verify shapes of the data\n",
    "print(\"train_images shape:\", train_images.shape)\n",
    "print(\"train_masks shape:\", train_masks.shape)\n",
    "print(\"test_images shape:\", test_images.shape)\n",
    "print(\"test_masks shape:\", test_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2811264-538b-4f60-899b-2129435ec060",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 16\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    validation_data=(x_valid, y_valid), \n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=[model_checkpoint, reduce_lr],\n",
    "                    shuffle=True,\n",
    "                    verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389cf97a-8473-4e0d-967b-ddd80762d5fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_kernel",
   "language": "python",
   "name": "torch_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
